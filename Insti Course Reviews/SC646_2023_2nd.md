---
layout: page
title: SC 646 - Distributed Optimization and Machine Learning
subtitle: Sameep Chattopadhyay, 2025 (Dual) 
cover-img: assets/img/Cover_study.jpg
thumbnail-img: ""
share-img: ""
comments: true
tags: [Academic]
---

### Basic Information

- **Course Code**: SC 646
- **Course Name**: Distributed Optimization and Machine Learning
- **Course Offered In**: 2023-2024
- **Semester Season**: Spring
- **Instructors**: Prof Mayank Baranwal
- **Prerequisites**: Soft Prerequisite: Linear Algebra
Hard Prerequisite: None
- **Difficulty (1 being easy and 5 being tough)**: 1

### Course Content
First half of the course deals with the general convex optimization and the Lyapunov theory while the second half covers some basics of graph theory, before delving into distributed optimization and finally finishing with a couple of lectures on Federated Learning. Most of the content in the first half overlaps with the other optimization courses in the Institute, while the second half has totally unique content.

### Feedback on Lectures
The Instructor would often start from scratch and revises all the required background theory on linear algebra, convex operations and graphs during the lectures itself. For someone, who has already done a course on optimization, this might feel a bit boring and slow paced, but is helpful for other students who do not have much background of the same. The doubts were cleared during the lecture itself, or else the instructor was available for some time post lecture time too. All the lectures were recorded during live class, and later uploaded on CDEEP within a week. The students were also shared lecture notes after every class. The course was easy to follow for anyone who would either attend the lectures or watch the recorded videos.

### Feedback on Evaluations
The evaluation consisted of 3 assignments, all of them having some theoretical parts and some coding questions. The assignments were not very long but they required a significant time to figure out the problems and often needed us to sift through papers online. These assignments contributed to about 40 % of the total evaluation, and thus an important component of the course. The midsem and endsem had weightage of 20% and 40% respectively, with most of the questions coming from either the lecture notes or the assignments. The exams were closed book, and mostly had the proofs that were covered in the lectures. Any student who would have seen the lectures and completed the assignments on their could score decently well in these exams. 

Lastly, the grading was very lenient as compared to most of the courses in the institute, and thus it was not very hard to obtain a good grade in this course

### Study Material and Resources
Most of the course content has been very recently developed, and thus there are not many books on these. The Instructor cites the papers that he is referring to in the lecture, so they can be of some help. A set of slides from a CMU course were pretty helpful to me - https://www.cs.cmu.edu/~pradeepr/convexopt/Lecture_Slides/ .

### Follow-up Courses
None

### Final Takeaway
A good course for someone who wants to learn about distributed system and optimization but does not have much background in either. The course is overall pretty light and has liberal grading.

